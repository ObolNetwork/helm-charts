OpenClaw is now installed.

Namespace: {{ .Release.Namespace }}
Service: {{ include "openclaw.fullname" . }}
Port: {{ .Values.service.port }}

Gateway token:
  kubectl get secret -n {{ .Release.Namespace }} {{ include "openclaw.secretsName" . }} -o jsonpath='{.data.{{ .Values.secrets.gatewayToken.key }}}' | base64 --decode

{{- if .Values.httpRoute.enabled }}

HTTPRoute is enabled. Access OpenClaw at:
{{- range .Values.httpRoute.hostnames }}
  http://{{ . }}
{{- end }}

{{- else if .Values.ingress.enabled }}

Ingress is enabled. Access OpenClaw at:
{{- range $host := .Values.ingress.hosts }}
  {{- range .paths }}
  http{{ if $.Values.ingress.tls }}s{{ end }}://{{ $host.host }}{{ .path }}
  {{- end }}
{{- end }}

{{- else }}

Port-forward for local access:
  export POD_NAME=$(kubectl get pods -n {{ .Release.Namespace }} -l "app.kubernetes.io/name={{ include "openclaw.name" . }},app.kubernetes.io/instance={{ .Release.Name }}" -o jsonpath="{.items[0].metadata.name}")
  kubectl -n {{ .Release.Namespace }} port-forward $POD_NAME 18789:{{ .Values.service.port }}
  open http://127.0.0.1:18789

{{- end }}

Next steps:
{{- if and .Values.models.ollama.enabled (not .Values.models.anthropic.enabled) (not .Values.models.openai.enabled) }}
  You are using the default Ollama provider. To configure a cloud LLM provider:
    obol llm configure --provider=anthropic --api-key=<your-key>
    obol openclaw setup {{ .Release.Name }}
{{- end }}
{{- if not (or .Values.models.ollama.enabled .Values.models.anthropic.enabled .Values.models.openai.enabled) }}
  WARNING: No model providers are enabled. Configure at least one provider:
    obol llm configure --provider=anthropic --api-key=<your-key>
    obol openclaw setup {{ .Release.Name }}
{{- end }}
  Dashboard:  open the URL above and enter your gateway token
  CLI docs:   obol openclaw --help
