# Example values for deploying an Aztec sequencer on mainnet
# A sequencer produces blocks and attestations for the network
#
# IMPORTANT: This example assumes you have deployed:
# 1. Geth execution client (see geth-mainnet.yaml example)
# 2. Prysm/Lighthouse consensus client (see prysm-mainnet.yaml example)
#
# Deploy those L1 clients first, then deploy this sequencer

# Deployment role
role: sequencer

# Sequencer-specific configuration
sequencer:
  # OPTION 1: Inline private key (NOT recommended for production)
  # Use --set sequencer.attesterPrivateKey="0xYOUR_KEY" when deploying
  # attesterPrivateKey: ""

  # OPTION 2: External Kubernetes secret (RECOMMENDED for production)
  # Create secret first:
  #   kubectl create secret generic aztec-sequencer-mainnet-keystore \
  #     --from-file=keystore.json=keystore.json -n aztec
  # Then reference it:
  attesterPrivateKeySecretName: "aztec-sequencer-mainnet-keystore"

  # REQUIRED: Aztec address (64-character hex string) to receive unburnt transaction fees
  # WARNING: If not set, the node will crash with "Invalid AztecAddress length 0"
  feeRecipient: "0x0000000000000000000000000000000000000000000000000000000000000000"

# Network to connect to
network: mainnet
networkName: mainnet

# Container image configuration
image:
  repository: aztecprotocol/aztec
  tag: 2.1.5  # For production, pin to specific version like "2.1.4"
  pullPolicy: Always  # Use Always for auto-updates, or IfNotPresent for pinned versions

# Node configuration
node:
  replicas: 1

  # Log level configuration
  # Format: "default_level; override: component, other_component"
  logLevel: "info"

  # Startup command - REQUIRED for sequencer
  # These flags tell Aztec to run as a full sequencer with archiver
  startCmd:
    - --node
    - --archiver
    - --sequencer
    - --network
    - mainnet  # WARNING: Must match network field above

  # L1 Ethereum configuration
  # Point to your deployed Geth + Prysm/Lighthouse stack
  # These should match the service names from your L1 deployment
  # Pattern: <service-name>.<namespace>.svc.cluster.local:<port>
  l1ExecutionUrls:
    - "http://geth-mainnet.l1.svc.cluster.local:8545"
  l1ConsensusUrls:
    - "http://prysm-mainnet.l1.svc.cluster.local:3500"

  # Resource allocation
  # Aztec minimum requirements: 2-4 cores, 16GB RAM, 1TB NVMe SSD, 25 Mbps network
  # These values provide headroom for production workloads
  resources:
    requests:
      cpu: "4"       # 4 cores (minimum: 2-4 cores)
      memory: "16Gi" # 16GB (minimum: 16GB)
    limits:
      cpu: "8"       # 8 cores for burst capacity
      memory: "32Gi" # 32GB for optimal performance

  # Storage configuration
  storage:
    dataDirectory: /data
    dataStoreMapSize: "134217728"  # 128 GB
    worldStateMapSize: "134217728"  # 128 GB

  # Startup probe - sequencers need extended startup time
  # 60s * 60 = 1 hour max startup time
  startupProbe:
    periodSeconds: 60
    failureThreshold: 60

# Persistence
# Aztec minimum requirement: 1TB NVMe SSD
persistence:
  enabled: true
  size: 1000Gi  # 1TB recommended for mainnet
  storageClassName: premium-rwo  # Use NVMe-backed storage class
  accessModes:
    - ReadWriteOnce
  selector: {}

# Networking configuration
# Choose ONE of the following networking strategies:

# OPTION 1: Host networking (recommended for best P2P performance)
# This allows direct binding to node's network interface
hostNetwork: true

# OPTION 2: NodePort (alternative if host networking not available)
# Uncomment if using NodePort instead of hostNetwork
# hostNetwork: false
# service:
#   p2p:
#     nodePortEnabled: true
#     nodePort: 30400

# Service configuration
service:
  httpPort: 8080

  # P2P networking
  p2p:
    enabled: true
    nodePortEnabled: false  # Not needed with hostNetwork
    port: 40400
    announcePort: 40400

  # Admin API for debugging and metrics
  admin:
    enabled: true
    port: 8081

# Pod annotations (optional - for auto-updates with keel.sh)
# podAnnotations:
#   keel.sh/policy: force
#   keel.sh/pollSchedule: "@every 10m"
#   keel.sh/trigger: poll
