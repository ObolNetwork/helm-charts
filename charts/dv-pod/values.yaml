# Default values for dv-pod.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Sub-chart configurations for execution and consensus layers
erigon:
  enabled: false
  extraArgs:
    - --chain=hoodi
    - --beacon.api=beacon,builder,config,debug,node,validator,lighthouse
  persistence:
    enabled: true
    size: 200Gi
    # storageClassName: "your-storage-class" # Uncomment and set if you have a specific storage class
  # Add any erigon specific default values here if needed in the future



# -- Validator client configuration
validatorClient:
  # -- Enable the validator client container
  enabled: true
  # -- Type of validator client to use
  # Options: lighthouse, teku, nimbus, lodestar
  type: "lighthouse"
  
  # -- Image configuration for validator client
  image:
    # Repository will be set based on the type selected
    repository: ""
    tag: ""
    pullPolicy: IfNotPresent
  
  # -- Validator client specific configuration
  config:
    # -- Graffiti to include in proposed blocks
    graffiti: "DV-Pod"
    # -- Additional CLI arguments for the validator client
    extraArgs: []
  
  # -- Resource limits and requests for validator client
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi
  
  # -- Validator keystores configuration
  keystores:
    # -- Name of the Secret containing validator keystores
    # If provided, skip keystore generation and use existing keys
    # The secret should contain keystore-*.json and keystore-*.txt files
    # Example: kubectl create secret generic validator-keys --from-file=keystore-0.json --from-file=keystore-0.txt
    secretName: ""
    
    # -- Automatically create secret from DKG-generated keystores
    # Only applies when secretName is empty and DKG runs successfully
    autoCreate: true

# -- Charon image repository, pull policy, and tag version
image:
  repository: obolnetwork/charon
  pullPolicy: IfNotPresent
  tag: v1.5.1

# -- Credentials to fetch images from private registry
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []

# -- Provide a name in place of lighthouse for `app:` labels
nameOverride: ""

# -- Provide a name to substitute for the full names of resources
fullnameOverride: ""

# -- Pod annotations
podAnnotations:
  # Pod Security Standards annotations
  # These annotations enforce security policies at the pod level
  # https://kubernetes.io/docs/concepts/security/pod-security-standards/
  # Note: We use "baseline" instead of "restricted" because the prepare-validator-data
  # init container needs to run as root to set proper file ownership
  pod-security.kubernetes.io/enforce: "baseline"
  pod-security.kubernetes.io/enforce-version: "latest"
  pod-security.kubernetes.io/audit: "baseline"
  pod-security.kubernetes.io/audit-version: "latest"
  pod-security.kubernetes.io/warn: "baseline"
  pod-security.kubernetes.io/warn-version: "latest"

# -- Service account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
serviceAccount:
  # -- Specifies whether a service account should be created
  enabled: true
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account to use. If not set and create is true, a name is generated using the default template
  name: ""

# -- The security context for pods
# @default -- See `values.yaml`
# Note: This must be set to null or omit runAsNonRoot to allow the prepare-validator-data
# init container to run as root for setting file permissions
securityContext: {}

# -- The security context for containers
# @default -- See `values.yaml`
containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false  # Charon needs to write to data directory
  capabilities:
    drop:
    - ALL

# -- RBAC configuration.
## ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
rbac:
  # -- Specifies whether RBAC resources are to be created
  enabled: true
  # -- The name of the cluster role to use. If not set and create is true, a name is generated using the fullname template
  name: ""
  # -- Required ClusterRole rules
  clusterRules:
    # --  Required to obtain the nodes external IP
    - apiGroups: [""]
      resources:
      - "nodes"
      verbs:
      - "get"
      - "list"
      - "watch"
  # -- Required Role rules
  rules:
    # -- Required to get information about the serices nodePort.
    - apiGroups: [""]
      resources:
      - "services"
      verbs:
      - "get"
      - "list"
      - "watch"

# -- Charon service ports
service:
  # -- Headless service to create DNS for each statefulset instance
  clusterIP: None
  ports:
    validatorApi:
      name: validator-api
      port: 3600
      protocol: TCP
      targetPort: 3600
    p2pTcp:
      name: p2p-tcp
      port: 3610
      protocol: TCP
      targetPort: 3610
    monitoring:
      name: monitoring
      port: 3620
      protocol: TCP
      targetPort: 3620

# -- Pod resources limits and requests
resources:
  # Default resource limits and requests for the main Charon container
  # These values are suitable for production use and can be adjusted based on your needs
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

# -- Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

# -- Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: {}

# -- Affinity for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
## Example:
## affinity:
##   podAntiAffinity:
##     requiredDuringSchedulingIgnoredDuringExecution:
##     - labelSelector:
##         matchExpressions:
##         - key: app.kubernetes.io/name
##           operator: In
##           values:
##           - charon
##       topologyKey: kubernetes.io/hostname
##
affinity: {}

# -- Used to assign priority to pods
## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
priorityClassName: ""

# -- Enable pod disruption budget
## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
podDisruptionBudget:
  minAvailable: ""
  enabled: true

# -- allows you to configure and disable automated rolling updates for containers, labels, resource request/limits, and annotations for the Pods in a StatefulSet.
updateStrategy: RollingUpdate

charon:
  # -- The Ethereum address of this operator. This MUST be provided by the user.
  operatorAddress: ""  # Required - set your operator address here
  
  # -- Beacon node endpoints (used when sub-charts are disabled)
  # These will be used by both Charon and the validator client
  beaconNodeEndpoints: 
    - "http://beacon-node:5052"
  
  # -- Execution client RPC endpoint URL (e.g., your Ethereum execution client)
  # Note: Charon currently only supports a single execution endpoint
  executionClientRpcEndpoint: ""
  
  # -- Fallback beacon node endpoints (optional)
  # These will be used if the primary beaconNodeEndpoints are unavailable
  fallbackBeaconNodeEndpoints: []
  # Example:
  # - "https://ethereum-beacon-api.publicnode.com"
  # - "http://backup-beacon-2:5052"
  
  # -- Beacon node authentication headers
  # WARNING: These headers will be sent to ALL beacon nodes, which could leak credentials
  # Format: "Authorization=Basic <base64_encoded_credentials>"
  # To generate: echo -n "username:password" | base64
  # Example: "Authorization=Basic am9objpkb2U="
  beaconNodeHeaders: ""
  
  # -- Optional: Name of the secret containing beacon node headers
  # Use this for secure credential storage instead of beaconNodeHeaders
  # The secret should contain a key specified by beaconNodeHeadersSecretKey
  beaconNodeHeadersSecretName: ""
  
  # -- Key within the beacon node headers secret
  beaconNodeHeadersSecretKey: "headers"
  # -- Configuration for the DKG Sidecar init container
  # This init container orchestrates the Distributed Key Generation (DKG) process for Charon clusters.
  #
  # The sidecar has three operating modes:
  # 1. If cluster-lock.json exists: Exits immediately (cluster already initialized)
  # 2. If cluster-definition.json exists: Attempts DKG with the existing definition
  # 3. If neither exists: Polls the Obol API for cluster invites and runs DKG when ready
  #
  # To provide a pre-existing cluster-lock and skip DKG:
  # 1. Create a configMap: kubectl create configmap cluster-lock --from-file=cluster-lock.json
  # 2. The sidecar will detect the lock file and exit, allowing Charon to start immediately
  #
  # To provide a cluster-definition without running DKG through the API:
  # 1. Mount your cluster-definition.json in /charon-data/
  # 2. The sidecar will run 'charon dkg' to generate the cluster-lock.json
  #
  # Note: When providing a pre-existing cluster-lock.json, you must also ensure
  # the associated validator keys are available in the charon-data volume.
  dkgSidecar:
    enabled: true
    image:
      # -- Image repository for the DKG sidecar
      repository: "ghcr.io/obolnetwork/charon-dkg-sidecar"
      tag: "v1.0.0"
      pullPolicy: "IfNotPresent"
    # -- Initial delay in seconds before the first retry of a polling cycle.
    initialRetryDelaySeconds: 10
    # -- Maximum delay in seconds for exponential backoff between polling cycles.
    maxRetryDelaySeconds: 300
    # -- Factor by which the retry delay increases after each polling cycle (e.g., 2 for doubling).
    retryDelayFactor: 2
    # -- Delay in seconds between polling retries
    retryDelaySeconds: 10
    # -- Page limit for API calls when fetching cluster definitions
    pageLimit: 10
    # -- API endpoint for the Obol network to fetch cluster definitions
    apiEndpoint: "https://api.obol.tech"
    # -- Resources for the cluster poller init container
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
    # -- Service account settings for test pods
    serviceAccount:
      create: true # Whether to specify the test service account for test pods

  enrJob:
    # -- Enable or disable the Kubernetes Job that generates/manages the ENR.
    # Note: This is typically not needed as the job automatically detects existing secrets.
    # The job will check if the ENR secret already exists and skip generation if found.
    # Only set to false for advanced use cases where you need to completely disable the job.
    enabled: true

  # --- Configuration for ENR management ---
  enr:
    # -- Provide the ENR private key directly (hex format, e.g., 0x...). 
    # If set, 'generate' and 'existingSecret' are ignored.
    privateKey: ""

    # -- Point to an existing Kubernetes secret that holds the ENR private key.
    # If 'privateKey' above is not set and this 'existingSecret.name' is provided, 'generate' is ignored.
    # NOTE: If not set, the chart will automatically check for a secret named 'charon-enr-private-key'
    # (configurable via secrets.defaultEnrPrivateKey) and use it if it exists.
    existingSecret:
      name: ""           # Name of the K8s secret (e.g., "my-dv-pod-enr-secret")
      dataKey: "private-key" # Key in the secret's 'data' field holding the private key hex string.

    # -- Enable automatic generation of an ENR private key.
    # This is active only if 'privateKey' and 'existingSecret.name' are NOT set.
    # The generated key will be stored in a secret (e.g., "{{ .Release.Name }}-dv-pod-enr-key") 
    # with data keys 'private-key' (for the hex key) and 'public-enr' (for the ENR string).
    generate:
      enabled: true
      image:
        repository: "obolnetwork/charon"
        # Specify a stable tag, e.g., v1.4.2 or check for latest on Docker Hub
        tag: "v1.5.1"
        pullPolicy: "IfNotPresent"
      # -- Image to use for kubectl operations within the ENR generation job
      # This image must contain a compatible kubectl binary.
      kubectlImage:
        repository: "bitnami/kubectl"
        tag: "1.33.3" # Pinned to match stable Kubernetes v1.33.3
        pullPolicy: "IfNotPresent"

  # --- Charon command line options ---
  ## ref: https://docs.obol.tech/docs/charon/charon_cli_reference
  
  # -- Enables the builder api. Will only produce builder blocks. Builder API must also be enabled on the validator client. Beacon node must be connected to a builder-relay to access the builder network.
  builderApi: ""
  
  # -- Minimum feature set to enable by default: alpha, beta, or stable. Warning: modify at own risk. (default "stable")
  featureSet: "stable"

  # -- Comma-separated list of features to disable, overriding the default minimum feature set.
  featureSetDisable: ""

  # -- Comma-separated list of features to enable, overriding the default minimum feature set.
  featureSetEnable: ""

  # -- The path to the cluster lock file defining distributed validator cluster. (default ".charon/cluster-lock.json")
  lockFile: "/charon-data/cluster-lock.json"

  # -- Log format; console, logfmt or json (default "console")
  logFormat: "json"

  # -- Log level; debug, info, warn or error (default "info")
  logLevel: "info"

  # --  Enables sending of logfmt structured logs to these Loki log aggregation server addresses. This is in addition to normal stderr logs.
  lokiAddresses: ""
  
  # -- Service label sent with logs to Loki.
  lokiService: ""

  # -- Listening address (ip and port) for the monitoring API (prometheus, pprof). (default "127.0.0.1:3620")
  monitoringAddress: "0.0.0.0:3620"

  # -- Disables cluster definition and lock file verification.
  noVerify: false

  # -- Comma-separated list of CIDR subnets for allowing only certain peer connections. Example: 192.168.0.0/16 would permit connections to peers on your local network only. The default is to accept all connections.
  p2pAllowlist: ""

  # -- Comma-separated list of CIDR subnets for disallowing certain peer connections. Example: 192.168.0.0/16 would disallow connections to peers on your local network. The default is to accept all connections.
  p2pDenylist: ""

  # -- Disables TCP port reuse for outgoing libp2p connections.
  p2pDisableReuseport: ""
  
  # --  The DNS hostname advertised by libp2p. This may be used to advertise an external DNS.
  p2pExternalHostname: ""

  # -- The IP address advertised by libp2p. This may be used to advertise an external IP.
  p2pExternalIp: ""

  # -- Comma-separated list of libp2p relay URLs or multiaddrs.
  # Default list is provided by libp2p and may change over time
  p2pRelays: ""

  # -- Comma-separated list of listening TCP addresses (ip and port) for libP2P traffic. Empty default doesn't bind to local port therefore only supports outgoing connections.
  p2pTcpAddress: "0.0.0.0:3610"
  
  # -- Listening address (ip and port) for validator-facing traffic proxying the beacon-node API. (default "127.0.0.1:3600")
  validatorApiAddress: "0.0.0.0:3600"

  # -- If enabled, it will set p2pExternalHostname value to the pod name and enable direct connection between cluster nodes.
  # This is useful for deployments where pods can directly communicate with each other.
  directConnectionEnabled: "true"

  # -- Path within the Charon container where the ENR private key file will be mounted.
  privateKeyFile: "/data/charon-enr-private-key"
  

# -- Kubernetes secrets names that might be used as suffixes or for other purposes.
# For the ENR, the secret name is either defined in 'charon.enr.existingSecret.name' 
# or generated by the ENR job (e.g., {{ .Release.Name }}-dv-pod-enr-key).
secrets:
  # -- Suffix for ENR private key secret (used internally by templates)
  enrPrivateKey: "charon-enr-private-key"
  # -- Default ENR private key secret name to check for auto-detection
  # If this secret exists in the namespace, it will be used automatically
  # unless explicitly overridden by charon.enr.existingSecret.name
  defaultEnrPrivateKey: "charon-enr-private-key"

# -- Kubernetes configMaps names for non-sensitive configuration data.
configMaps:
  # -- Name of the ConfigMap containing the cluster-lock.json file
  # Set this to the name of an existing ConfigMap to skip the DKG process.
  # Example: If you have created a ConfigMap named "my-cluster-lock":
  #   kubectl create configmap my-cluster-lock --from-file=cluster-lock.json
  # Then set: clusterlock: "my-cluster-lock"
  # If not set or if the ConfigMap doesn't exist, the DKG process will run.
  # Note: ConfigMaps support larger file sizes than Secrets (up to 1MB compressed), making them
  # more suitable for cluster-lock files which can be several megabytes.
  clusterlock: ""
  

# -- Persistence configuration for DKG artifacts and charon data
persistence:
  # -- Enable persistence using a PersistentVolumeClaim.
  enabled: true
  # -- Storage class for the PVC. If undefined or null, the default storage class will be used.
  # storageClassName: "" # e.g., "standard", "gp2"
  # -- Access modes for the PVC. Must be a list. Default: ["ReadWriteOnce"].
  accessModes:
    - ReadWriteOnce
  # -- Size of the PVC for charon-data.
  size: 1Gi
  # -- Size of the PVC for validator-data.
  # Validator data includes slashing protection DB and other validator state.
  # NOTE: Validator data ALWAYS uses persistent storage to prevent slashing,
  # even if persistence.enabled is false for charon-data.
  validatorDataSize: 500Mi
  # -- Optional: Annotations for the PVC
  # annotations: {}
  #   key: value


# -- Prometheus Service Monitor
## ref: https://github.com/coreos/prometheus-operator
serviceMonitor:
  # -- If true, a ServiceMonitor CRD is created for a prometheus operator. https://github.com/coreos/prometheus-operator
  # TODO: SWITCH BACK TO ON FOR PRODUCTION
  enabled: false
  # -- Path to scrape
  path: /metrics
  # -- Alternative namespace for ServiceMonitor
  namespace: null
  # -- Additional ServiceMonitor labels
  labels: {}
  # -- Additional ServiceMonitor annotations
  annotations: {}
  # -- ServiceMonitor scrape interval
  interval: 1m
  # -- ServiceMonitor scheme
  scheme: http
  # -- ServiceMonitor TLS configuration
  tlsConfig: {}
  # -- ServiceMonitor scrape timeout
  scrapeTimeout: 30s
  # -- ServiceMonitor relabelings
  relabelings: []

# -- Configure liveness probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  enabled: false
  initialDelaySeconds: 10
  periodSeconds: 5
  httpGet:
    path: /livez

# -- Configure readiness probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
readinessProbe:
  enabled: false
  initialDelaySeconds: 5
  periodSeconds: 3
  httpGet:
    path: /readyz

# -- Central Monitoring
centralMonitoring:
  # -- Specifies whether central monitoring should be enabled
  enabled: false
  # -- https endpoint to obol central prometheus 
  promEndpoint: "https://vm.monitoring.gcp.obol.tech/write"
  # -- The authentication token to the central prometheus
  token: ""

# -- Configuration for running Helm tests.
# These values are typically only used when `helm test` is run.
tests:
  # -- The operator address to use for DKG sidecar tests.
  # This should be a valid Ethereum address (0x...).
  dkgSidecar:
    enabled: true
    operatorAddress: "0x3D1f0598943239806A251899016EAf4920d4726d"
    mockApi:
      port: 3001
      image:
        pullPolicy: Always # Or Always, Never as needed
    # -- Service account settings for test pods
    serviceAccount:
      create: true # Whether to specify the test service account for test pods
    # -- Host network setting for dkgSidecar test pods
    hostNetwork: false 

# -- Configuration for the DKG Mock API Server used in tests.
# This is relevant if you are using the mock server deployment included in this chart for testing.

# -- NetworkPolicy configuration for pod network isolation
networkPolicy:
  # -- Enable NetworkPolicy to restrict network traffic
  enabled: false
  # -- Selector for validator client pods that can access the validator API
  validatorClientSelector: {}
    # app: validator-client
  # -- Namespace selector for validator client access
  validatorClientNamespaceSelector: {}
    # name: validators
  # -- Monitoring configuration
  monitoring:
    # -- Enable access from monitoring tools
    enabled: true
    # -- Pod selector for monitoring tools (e.g., Prometheus)
    podSelector: {}
      # app: prometheus
    # -- Namespace selector for monitoring namespace
    namespaceSelector: {}
      # name: monitoring
  # -- Beacon node configuration
  beaconNodes:
    # -- Enable egress to beacon nodes
    enabled: true
    # -- Pod selector for beacon nodes
    podSelector: {}
    # -- Namespace selector for beacon nodes
    namespaceSelector: {}
    # -- IP block for external beacon nodes
    ipBlock: {}
      # cidr: 0.0.0.0/0
      # except:
      #   - 169.254.169.254/32
    # -- Port for beacon node connections (leave empty for any port)
    port: null
  # -- Obol API configuration
  obolApi:
    # -- Enable egress to Obol API for DKG
    enabled: true
    # -- CIDR block for Obol API access
    cidr: "0.0.0.0/0"
    # -- IP ranges to exclude
    except: []
  # -- Custom ingress rules to add
  customIngress: []
  # -- Custom egress rules to add
  customEgress: []
