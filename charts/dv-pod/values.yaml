# Default values for dv-pod.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# -- Kubernetes secrets names
# For the ENR, the secret name is either defined in 'charon.enr.existingSecret.name'
# or generated by the ENR job (e.g., {{ .Release.Name }}-dv-pod-enr-key).
secrets:
  # -- Default ENR private key secret name to check for auto-detection
  # IMPORTANT: This secret MUST exist in the same namespace as the Helm release
  # If this secret exists in the release namespace, it will be used automatically
  # unless explicitly overridden by charon.enr.existingSecret.name
  # The ENR job will NOT check other namespaces to prevent unexpected behavior
  defaultEnrPrivateKey: "charon-enr-private-key"

# -- Kubernetes configMaps names for non-sensitive configuration data.
configMaps:
  # -- Name of the ConfigMap containing the cluster-lock.json file
  # Set this to the name of an existing ConfigMap to skip the DKG process.
  # Example: kubectl create configmap cluster-lock --from-file=.charon/cluster-lock.json
  # Then set: clusterLock: "cluster-lock"
  # If not set or if the ConfigMap doesn't exist, the DKG process will run.
  # NOTE: For large cluster-lock files (>1MB), use charon.lockHash instead
  # clusterLock: "cluster-lock"
  clusterLock: ""

# This code block control's charon's configuration, including the DKG initcontainer
charon:
  # -- The Ethereum address of this operator. This MUST be provided by the user to use the auto-dkg functionality.
  operatorAddress: ""  # Set your operator address here. This will be used to sign approvals to DKG invites on the Obol Launchpad.
  
  # -- The nickname this Charon appears as in monitoring and its peer's logs. Maximum 32 characters.
  nickname: ""

  # -- Beacon node endpoints
  # The default is the Obol Stack full node address. Change this to your own consensus client beacon API(s).
  # All beacon nodes specified here will be called for every charon request, if you want to leverage fallback behaviour
  # place secondary beacon nodes in charon.fallbackBeaconNodeEndpoints[]
  beaconNodeEndpoints: 
    - "http://l1-full-node-beacon.l1.cluster.svc.local:5052"
  
  # -- Fallback beacon node endpoints (optional)
  # These will be used if the primary beaconNodeEndpoints are unavailable
  fallbackBeaconNodeEndpoints:
  # Example:
    - "https://ethereum-beacon-api.publicnode.com"
  # - "http://backup-beacon-2:5052"
  
  # -- Beacon node authentication headers
  # WARNING: These headers will be sent to ALL beacon nodes, which could leak credentials
  # Format: "Authorization=Basic <base64_encoded_credentials>"
  # To generate: echo -n "username:password" | base64
  # Example: "Authorization=Basic am9objpkb2U="
  beaconNodeHeaders: ""
  
  # -- Optional: Name of the secret containing beacon node headers
  # Use this for secure credential storage instead of beaconNodeHeaders
  # The secret should contain a key specified by charon.beaconNodeHeadersSecretKey
  beaconNodeHeadersSecretName: ""
  
  # -- Optional: Key within the charon.beaconNodeHeaders secret to read from
  beaconNodeHeadersSecretKey: "headers"

  # -- Optional: Execution client RPC endpoint URL (e.g., your Ethereum execution client)
  # Note: Charon currently only supports a single execution endpoint
  # This is only needed if an operator in the cluster uses a smart contract wallet for an operator signature. 
  # If that does not apply to this cluster, this field can be left unset.
  executionClientRpcEndpoint: ""

  # --- Charon command line options ---
  ## ref: https://docs.obol.tech/docs/learn/charon/charon-cli-reference
  
  # -- Enables the builder api (MEV) for validator proposals.
  # The Builder API must also be enabled on your beacon node client(s). 
  # The Beacon node must either be connected to a mev sidecar or a mev relay to access the builder network.
  # More documentation about MEV can be found [here](https://docs.obol.org/advanced-and-troubleshooting/advanced/enable-mev).
  builderApi: "true"
  
  # -- Minimum feature set to enable by default: alpha, beta, or stable. Warning: modify at own risk. (default "stable")
  featureSet: "stable"

  # -- Comma-separated list of features to disable, overriding the default minimum feature set.
  featureSetDisable: ""

  # -- Comma-separated list of features to enable, overriding the default minimum feature set.
  featureSetEnable: ""

  # -- The path on the pod to the cluster lock file that definesthe distributed validator cluster. (default "/charon-data/cluster-lock.json")
  lockFile: "/charon-data/cluster-lock.json"

  # -- Cluster lock hash for large cluster-lock files (>1MB)
  # When provided, the DKG sidecar will fetch the full cluster-lock from Obol API using this hash
  # Extract the hash from a cluster-lock.json using: `jq -r '.lock_hash' cluster-lock.json`
  # Alternative to providing the full cluster-lock.json file via the configMaps.clusterLock value
  lockHash: ""

  # -- Log format; console, logfmt or json (default "json")
  logFormat: "json"

  # -- Log level; debug, info, warn or error (default "info")
  logLevel: "info"

  # --  Enables sending of logfmt structured logs to these Loki log aggregation server addresses. This is in addition to normal stderr logs.
  lokiAddresses: ""
  
  # -- Service label sent with logs to Loki.
  lokiService: ""

  # -- Listening address (ip and port) for the monitoring API (prometheus, pprof). (default "127.0.0.1:3620")
  monitoringAddress: "0.0.0.0:3620"

  # -- Disables cluster definition and lock file verification.
  noVerify: false

  # -- Comma-separated list of CIDR subnets for allowing only certain peer connections. Example: 192.168.0.0/16 would permit connections to peers on your local network only. The default is to accept all connections.
  p2pAllowlist: ""

  # -- Comma-separated list of CIDR subnets for disallowing certain peer connections. Example: 192.168.0.0/16 would disallow connections to peers on your local network. The default is to accept all connections.
  p2pDenylist: ""

  # -- Disables TCP port reuse for outgoing libp2p connections.
  p2pDisableReuseport: ""
  
  # --  The DNS hostname advertised by libp2p. This may be used to advertise an external DNS.
  p2pExternalHostname: ""

  # -- The IP address advertised by libp2p. This may be used to advertise an external IP.
  p2pExternalIp: ""

  # -- Comma-separated list of libp2p relay URLs or multiaddrs.
  # Default list is provided by libp2p and may change over time
  p2pRelays: ""

  # -- Comma-separated list of listening TCP addresses (ip and port) for libP2P traffic. Empty default doesn't bind to local port therefore only supports outgoing connections.
  p2pTcpAddress: "0.0.0.0:3610"
  
  # -- Listening address (ip and port) for validator-facing traffic proxying the beacon-node API. (default "127.0.0.1:3600")
  validatorApiAddress: "0.0.0.0:3600"

  # -- If enabled, it will set p2pExternalHostname value to the pod name and enable direct connection between cluster nodes.
  # This is useful for deployments where pods can directly communicate with each other.
  directConnectionEnabled: "true"

  # -- Path within the Charon container where the ENR private key file will be mounted.
  privateKeyFile: "/data/charon-enr-private-key"
  
  # -- Configuration for the DKG Sidecar init container
  # This init container orchestrates the Distributed Key Generation (DKG) process for Charon clusters.
  #
  # The sidecar has three operating modes:
  # 1. If cluster-lock.json exists: Exits immediately (cluster already initialized)
  # 2. If cluster-definition.json exists: Attempts DKG with the existing definition
  # 3. If neither exists: Polls the Obol API for cluster invites and runs DKG when ready
  #
  # To provide a pre-existing cluster-lock and skip DKG:
  # 1. Create a configMap: kubectl create configmap cluster-lock --from-file=cluster-lock.json
  # 2. Change the `confiMaps.clusterLock: cluster-lock in your values.yaml file,
  #    or pass --set configMaps.clusterLock=cluster-lock in your helm install command.
  # 3. The sidecar will detect the mounted lock file and exit, allowing Charon to start immediately
  #
  # To provide a cluster-definition without running DKG through the API:
  # 1. Mount your cluster-definition.json in /charon-data/
  # 2. The sidecar will run 'charon dkg' to generate the cluster-lock.json
  #
  # Note: When providing a pre-existing cluster-lock.json, you must also ensure
  # the associated validator keys are available in the charon-data volume.
  dkgSidecar:
    enabled: true
    image:
      # -- Image repository for the DKG sidecar
      repository: "obolnetwork/charon-dkg-sidecar"
      tag: "main"
      pullPolicy: "IfNotPresent"
    # -- Initial delay in seconds before the first retry of a polling cycle.
    initialRetryDelaySeconds: 10
    # -- Maximum delay in seconds for exponential backoff between polling cycles.
    maxRetryDelaySeconds: 300
    # -- Factor by which the retry delay increases after each polling cycle (e.g., 2 for doubling).
    retryDelayFactor: 2
    # -- Delay in seconds between polling retries
    retryDelaySeconds: 10
    # -- Page limit for API calls when fetching cluster definitions
    pageLimit: 10
    # -- API endpoint for the Obol network to fetch cluster definitions
    apiEndpoint: "https://api.obol.tech"
    # -- Target cluster definition config hash for the DKG sidecar to partake in (optional).
    # Must be a valid 32-byte hex hash with 0x prefix (66 characters total).
    # Example: "0x7f0fd29abb11674b4e61000de26bff3600237aab0402427bd1409756665c2115"
    targetConfigHash: ""
    # -- Resources for the cluster poller init container
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 200m
        memory: 256Mi
    # -- Service account settings for test pods
    serviceAccount:
      create: true # Whether to specify the test service account for test pods

  enrJob:
    # -- Enable or disable the Kubernetes Job that generates/manages the ENR.
    # Note: This is typically not needed as the job automatically detects existing secrets.
    # The job will check if the ENR secret already exists and skip generation if found.
    # Only set to false for advanced use cases where you need to completely disable the job.
    enabled: true

  # --- Configuration for ENR management ---
  enr:
    # -- Provide the ENR private key directly (hex format, e.g., 0x...). 
    # If set, 'generate' and 'existingSecret' are ignored.
    privateKey: ""

    # -- Point to an existing Kubernetes secret that holds the ENR private key.
    # If 'privateKey' above is not set and this 'existingSecret.name' is provided, 'generate' is ignored.
    # NOTE: If not set, the chart will automatically check for a secret named 'charon-enr-private-key'
    # (configurable via secrets.defaultEnrPrivateKey) and use it if it exists.
    existingSecret:
      name: ""           # Name of the K8s secret (e.g., "my-dv-pod-enr-secret")
      # -- Key in the secret's 'data' field holding the private key hex string
      privateKeyDataKey: "charon-enr-private-key"
      # -- Key in the secret's 'data' field holding the public ENR string
      publicKeyDataKey: "enr"

    # -- Enable automatic generation of an ENR private key.
    # Only used if 'privateKey' and 'existingSecret.name' are not provided.
    # The generated key will be stored in a secret with data keys 'charon-enr-private-key' and 'enr'.
    generate:
      enabled: true
      # -- Annotations to add to the ENR generation job
      annotations: {}
      # -- Node selector for the ENR generation job
      nodeSelector: {}
      image:
        repository: "obolnetwork/charon"
        # Specify a stable tag, e.g., v1.6.0 or check for the latest release [here](https://github.com/ObolNetwork/charon/releases/).
        tag: "v1.7.1"
        pullPolicy: "IfNotPresent"
      # -- Image to use for kubectl operations within the ENR generation job
      # This image must contain a compatible kubectl binary.
      kubectlImage:
        repository: "bitnamisecure/kubectl"
        tag: "latest"
        pullPolicy: "IfNotPresent"

 
  
# -- Validator client configuration
validatorClient:
  # -- Enable the validator client container
  # If you want to use an externally managed validator client. 
  # Set this to false, and set your external validator to communicate with the 'validator-api' service created by this chart as if it were a beacon node API. 
  enabled: true
  # -- Type of validator client to use
  # Options: lighthouse, teku, prysm, nimbus, lodestar
  type: "lighthouse"
  
  # -- Image configuration for validator client
  # Repository and tag will be auto-selected based on validator client type if not specified
  image:
    repository: ""
    tag: ""
    pullPolicy: IfNotPresent
  
  # -- Validator client specific configuration
  config:
    # -- Graffiti to include in proposed blocks
    # Leaving it unset will result in charon defaults.
    graffiti: ""
    # -- Network configuration for validator client (e.g., mainnet, sepolia)
    # Used by some validator clients like Teku for network-specific configuration
    network: ""
    # -- Additional CLI arguments passed to the validator client (regardless of type)
    # (the minimum required mev flags are automatically included when you set .Values.charon.builderApi to true)
    extraArgs: []

    # -- Configuration for the prysm validator client specifically
    # Used to pass Prysm VC specific configuration
    # NOTE: By using Prysm, you automatically accept the Terms of Service: https://github.com/prysmaticlabs/prysm/blob/develop/TERMS_OF_SERVICE.md
    prysm:
      # -- Additional CLI arguments passed to the Prysm validator client only
      extraArgs: []
  
  # -- Resource limits and requests for validator client
  resources:
    limits:
      cpu: 1000m
      memory: 2Gi
    requests:
      cpu: 500m
      memory: 1Gi
  
  # -- Validator keystores configuration
  keystores:
    # -- Name of the Secret containing validator keystores
    # If provided, skip keystore generation and use existing keys
    # The secret should contain keystore-*.json and keystore-*.txt files
    # Example: kubectl create secret generic validator-keys --from-file=keystore-0.json --from-file=keystore-0.txt
    # secretName: "validator-keys"
    secretName: ""

# -- Central Monitoring
centralMonitoring:
  # -- Specifies whether central monitoring should be enabled
  enabled: false
  # -- https endpoint to obol central prometheus 
  promEndpoint: "https://vm.monitoring.gcp.obol.tech/write"
  # -- The authentication token to the central prometheus
  token: ""

# -- Charon image repository, pull policy, and tag version
image:
  repository: obolnetwork/charon
  pullPolicy: IfNotPresent
  tag: v1.7.1

# -- Chain ID for the network (1: Mainnet, 11155111: Sepolia, 560048: Hoodi)
# Used for DKG Ceremony
chainId: 1  # Default to Mainnet

# -- Credentials to fetch images from private registry
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []

# -- Global configuration that can be referenced across the chart
# Used for test configurations and shared settings
global:
  # -- Global annotations applied to resources
  annotations: {}

# -- Provide a name in place of charon for `app:` labels
nameOverride: ""

# -- Provide a name to substitute for the full names of resources
fullnameOverride: ""

# -- Pod annotations
podAnnotations:
  # Pod Security Standards annotations
  # These annotations enforce security policies at the pod level
  # https://kubernetes.io/docs/concepts/security/pod-security-standards/
  # Note: We use "baseline" instead of "restricted" because the prepare-validator-data
  # init container needs to run as root to set proper file ownership
  pod-security.kubernetes.io/enforce: "baseline"
  pod-security.kubernetes.io/enforce-version: "latest"
  pod-security.kubernetes.io/audit: "baseline"
  pod-security.kubernetes.io/audit-version: "latest"
  pod-security.kubernetes.io/warn: "baseline"
  pod-security.kubernetes.io/warn-version: "latest"

# -- Service account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
serviceAccount:
  # -- Specifies whether a service account should be created
  enabled: true
  # -- Annotations to add to the service account
  annotations: {}
  # -- The name of the service account to use. If not set, a name is generated using the default template
  name: ""
  # -- The name of the service account to use for test pods. If not set, uses the main service account
  nameTest: ""

# -- The security context for pods
# @default -- See `values.yaml`
# Note: This must be set to null or omit runAsNonRoot to allow the prepare-validator-data
# init container to run as root for setting file permissions
securityContext: {}

# -- The security context for containers
# @default -- See `values.yaml`
containerSecurityContext:
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: false  # Charon needs to write to data directory
  capabilities:
    drop:
    - ALL

# -- RBAC configuration.
## ref: https://kubernetes.io/docs/reference/access-authn-authz/rbac/
rbac:
  # -- Specifies whether RBAC resources are to be created
  enabled: true
  # -- The name of the cluster role to use. If not set and create is true, a name is generated using the fullname template
  name: ""
  # -- Required ClusterRole rules
  clusterRules:
    # --  Required to obtain the nodes external IP
    - apiGroups: [""]
      resources:
      - "nodes"
      verbs:
      - "get"
      - "list"
      - "watch"
  # -- Required Role rules
  rules:
    # -- Required to get information about the serices nodePort.
    - apiGroups: [""]
      resources:
      - "services"
      verbs:
      - "get"
      - "list"
      - "watch"

# -- Charon service ports
service:
  # -- Headless service to create DNS for each statefulset instance
  clusterIP: None
  ports:
    validatorApi:
      name: validator-api
      port: 3600
      protocol: TCP
      targetPort: 3600
    p2pTcp:
      name: p2p-tcp
      port: 3610
      protocol: TCP
      targetPort: 3610
    monitoring:
      name: monitoring
      port: 3620
      protocol: TCP
      targetPort: 3620

# -- Pod resources limits and requests
resources:
  # Default resource limits and requests for the main Charon container
  # These values are suitable for production use and can be adjusted based on your needs
  limits:
    cpu: 2000m
    memory: 4Gi
  requests:
    cpu: 1000m
    memory: 2Gi

# -- Node labels for pod assignment
## ref: https://kubernetes.io/docs/user-guide/node-selection/
nodeSelector: {}

# -- Tolerations for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
tolerations: {}

# -- Affinity for pod assignment
## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
##
## Example:
## affinity:
##   podAntiAffinity:
##     requiredDuringSchedulingIgnoredDuringExecution:
##     - labelSelector:
##         matchExpressions:
##         - key: app.kubernetes.io/name
##           operator: In
##           values:
##           - charon
##       topologyKey: kubernetes.io/hostname
##
affinity: {}

# -- Used to assign priority to pods
## ref: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/
priorityClassName: ""

# -- Enable pod disruption budget
## ref: https://kubernetes.io/docs/tasks/run-application/configure-pdb
podDisruptionBudget:
  minAvailable: ""
  enabled: true

# -- Maximum number of pods that can be unavailable for cluster threshold
# Used in pod disruption budget when minAvailable is not specified
clusterThreshold: ""

# -- allows you to configure and disable automated rolling updates for containers, labels, resource request/limits, and annotations for the Pods in a StatefulSet.
updateStrategy: RollingUpdate
  
# -- Persistence configuration for DKG artifacts and charon data
persistence:
  # -- Enable persistence using a PersistentVolumeClaim.
  enabled: true
  # -- Storage class for the PVC. If undefined or null, the default storage class will be used.
  # storageClassName: "" # e.g., "standard", "gp2"
  # -- Access modes for the PVC. Must be a list. Default: ["ReadWriteOnce"].
  accessModes:
    - ReadWriteOnce
  # -- Size of the PVC for charon-data.
  size: 1Gi
  # -- Size of the PVC for validator-data.
  # Validator data includes slashing protection DB and other validator state.
  # NOTE: Validator data ALWAYS uses persistent storage to prevent slashing,
  # even if persistence.enabled is false for charon-data.
  validatorDataSize: 500Mi
  # -- Annotations for the PVC
  # Example: annotations: {"volume.beta.kubernetes.io/storage-class": "fast"}
  annotations: {}


# -- Prometheus Service Monitor
## ref: https://github.com/coreos/prometheus-operator
serviceMonitor:
  # -- If true, a ServiceMonitor CRD is created for a prometheus operator. https://github.com/coreos/prometheus-operator
  # TODO: SWITCH BACK TO ON FOR PRODUCTION
  enabled: false
  # -- Path to scrape
  path: /metrics
  # -- Alternative namespace for ServiceMonitor
  namespace: null
  # -- Additional ServiceMonitor labels
  labels: {}
  # -- Additional ServiceMonitor annotations
  annotations: {}
  # -- ServiceMonitor scrape interval
  interval: 1m
  # -- ServiceMonitor scheme
  scheme: http
  # -- ServiceMonitor TLS configuration
  tlsConfig: {}
  # -- ServiceMonitor scrape timeout
  scrapeTimeout: 30s
  # -- ServiceMonitor relabelings
  relabelings: []

# -- Configure liveness probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
livenessProbe:
  enabled: false
  initialDelaySeconds: 10
  periodSeconds: 5
  httpGet:
    path: /livez
    # -- Port for liveness probe HTTP checks
    port: 3620

# -- Configure readiness probes
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
readinessProbe:
  enabled: false
  initialDelaySeconds: 5
  periodSeconds: 3
  httpGet:
    path: /readyz
    # -- Port for readiness probe HTTP checks
    port: 3620

# -- Configuration for running Helm tests.
# These values are typically only used when `helm test` is run.
tests:
  # -- The operator address to use for DKG sidecar tests.
  # This should be a valid Ethereum address (0x...).
  dkgSidecar:
    enabled: true
    operatorAddress: "0x3D1f0598943239806A251899016EAf4920d4726d"
    # -- Target config hash for testing (optional)
    # When set, enables test-target-config-hash-set.yaml test and passes hash to DKG sidecar tests
    targetConfigHash: "0x7f0fd29abb11674b4e61000de26bff3600237aab0402427bd1409756665c2115"
    mockApi:
      port: 3001
      image:
        pullPolicy: Always # Or Always, Never as needed
    # -- Service account settings for test pods
    serviceAccount:
      create: true # Whether to specify the test service account for test pods
    # -- Host network setting for dkgSidecar test pods
    hostNetwork: false

  # -- Validator keystore configuration test
  validatorKeystore:
    # -- Enable validator keystore configuration tests
    enabled: false
    # -- Validator client type to test (lodestar, lighthouse, teku, prysm, nimbus)
    # Currently lodestar, prysm, and nimbus are implemented
    validatorClientType: "lodestar"
    # -- Number of mock keystores to generate for testing
    mockKeystoreCount: 2

# -- Configuration for the DKG Mock API Server used in tests.
# This is relevant if you are using the mock server deployment included in this chart for testing.

# -- NetworkPolicy configuration for pod network isolation
networkPolicy:
  # -- Enable NetworkPolicy to restrict network traffic
  enabled: false
  # -- Selector for validator client pods that can access the validator API
  validatorClientSelector: {}
    # app: validator-client
  # -- Namespace selector for validator client access
  validatorClientNamespaceSelector: {}
    # name: validators
  # -- Monitoring configuration
  monitoring:
    # -- Enable access from monitoring tools
    enabled: true
    # -- Pod selector for monitoring tools (e.g., Prometheus)
    podSelector: {}
      # app: prometheus
    # -- Namespace selector for monitoring namespace
    namespaceSelector: {}
      # name: monitoring
  # -- Beacon node configuration
  beaconNodes:
    # -- Enable egress to beacon nodes
    enabled: true
    # -- Pod selector for beacon nodes
    podSelector: {}
    # -- Namespace selector for beacon nodes
    namespaceSelector: {}
    # -- IP block for external beacon nodes
    ipBlock: {}
      # cidr: 0.0.0.0/0
      # except:
      #   - 169.254.169.254/32
    # -- Port for beacon node connections (leave empty for any port)
    port: null
  # -- Obol API configuration
  obolApi:
    # -- Enable egress to Obol API for DKG
    enabled: true
    # -- CIDR block for Obol API access
    cidr: "0.0.0.0/0"
    # -- IP ranges to exclude
    except: []
  # -- Custom ingress rules to add
  customIngress: []
  # -- Custom egress rules to add
  customEgress: []
