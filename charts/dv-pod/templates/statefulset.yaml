{{- $root := . -}}
{{- include "dv-pod.validateValidatorClientType" . -}}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ include "dv-pod.fullname" . }}
  labels:
    {{- include "dv-pod.labels" . | nindent 4 }}
spec:
  serviceName: {{ include "dv-pod.fullname" . }}
  updateStrategy:
    type: {{ .Values.updateStrategy }}
  selector:
    matchLabels:
      {{- include "dv-pod.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        {{- include "dv-pod.selectorLabels" . | nindent 8 }}
    spec:
      {{- with concat .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- if or .Values.serviceAccount.enabled }}
      serviceAccountName: {{ include "dv-pod.serviceAccountName" . }}
      {{- end }}
      {{- if .Values.priorityClassName }}
      priorityClassName: {{ .Values.priorityClassName | quote }}
      {{- end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.securityContext }}
      securityContext:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      initContainers:
        # DKG Sidecar Init Container
        # This container orchestrates the Distributed Key Generation (DKG) process for Charon clusters.
        # 
        # Behavior:
        # 1. If /charon-data/cluster-lock.json exists: Exits immediately (cluster already initialized)
        # 2. If /charon-data/cluster-definition.json exists but no cluster-lock.json: Runs DKG with existing definition
        # 3. If neither exists: Polls Obol API for cluster invites and runs DKG when ready
        #
        # To skip this container entirely:
        # - Create a configMap with your cluster-lock.json and reference it in values.yaml
        # - The container will detect the existing lock file and exit gracefully
        {{- if .Values.charon.dkgSidecar.enabled }}
        - name: dkg-sidecar
          image: "{{ .Values.charon.dkgSidecar.image.repository }}:{{ .Values.charon.dkgSidecar.image.tag | default .Chart.AppVersion }}"
          imagePullPolicy: {{ .Values.charon.dkgSidecar.image.pullPolicy }}
          securityContext:
            runAsUser: 1000
            runAsGroup: 1000
          args:
            - "{{ .Values.charon.operatorAddress }}"
            - "/enr-from-job/{{ include "dv-pod.enrSecretPublicDataKey" . }}"
          {{- if .Values.charon.lockHash }}
          command:
            - sh
            - -c
            - |
              # Write lockHash to file for DKG sidecar
              echo "{{ .Values.charon.lockHash }}" > /charon-data/lockhash
              # Execute the original command with the original args
              exec node /app/dist/dkg-sidecar.js {{ .Values.charon.operatorAddress }} /enr-from-job/{{ include "dv-pod.enrSecretPublicDataKey" . }}
          {{- end }}
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: OUTPUT_DEFINITION_FILE
              value: "/charon-data/cluster-definition.json"
            - name: API_ENDPOINT
              value: "{{ .Values.charon.dkgSidecar.apiEndpoint }}"
            - name: INITIAL_RETRY_INTERVAL_SECONDS
              value: "{{ .Values.charon.dkgSidecar.initialRetryDelaySeconds }}"
            - name: MAX_RETRY_INTERVAL_SECONDS
              value: "{{ .Values.charon.dkgSidecar.maxRetryDelaySeconds }}"
            - name: BACKOFF_FACTOR
              value: "{{ .Values.charon.dkgSidecar.retryDelayFactor }}"
            - name: PAGE_LIMIT
              value: "{{ .Values.charon.dkgSidecar.pageLimit }}"
            - name: CHARON_NODE_ID_DIR
              value: "/charon-data"
            - name: CHARON_PRIVATE_KEY_FILE
              value: "/charon-data/charon-enr-private-key"
            - name: CHAIN_ID
              value: "{{ include "dv-pod.chainId" . }}"
            {{- if .Values.charon.dkgSidecar.targetConfigHash }}
            - name: TARGET_CONFIG_HASH
              value: "{{ .Values.charon.dkgSidecar.targetConfigHash }}"
            {{- end }}
          volumeMounts:
            - name: charon-enr-private-key
              mountPath: /enr-from-job
              readOnly: true
            - name: charon-data
              mountPath: /charon-data
            {{- if .Values.configMaps.clusterLock }}
            # Mount pre-existing cluster-lock if provided
            - name: cluster-lock
              mountPath: /charon-data/cluster-lock.json
              subPath: cluster-lock.json
              readOnly: true
            {{- end }}
            - name: charon-enr-private-key
              mountPath: /charon-data/charon-enr-private-key
              subPath: "{{ include "dv-pod.enrSecretDataKey" . }}"
          resources:
            {{- toYaml .Values.charon.dkgSidecar.resources | nindent 12 }}
        {{- end }}
        {{- if .Values.validatorClient.enabled }}
        # Keystore Import Init Container
        # This container prepares the validator data directory with proper ownership
        # It runs as root only to set ownership, then the actual import runs as non-root
        {{- if .Values.validatorClient.enabled }}
        - name: prepare-validator-data
          image: busybox:1.37.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          command: ["/bin/sh", "-c"]
          args:
            - |
              echo "Preparing validator data directory..."
              mkdir -p /validator-data
              chown -R 1000:1000 /validator-data
              echo "Directory prepared with correct ownership"
          volumeMounts:
            - name: validator-data
              mountPath: /validator-data
        {{- end }}
        # This container imports validator keystores for the validator client.
        # It handles different validator client formats and sources:
        # 1. Pre-existing keystores from a Secret
        # 2. DKG-generated keystores from /charon-data/validator_keys/
        - name: import-keystores
          image: busybox:1.37.0
          imagePullPolicy: IfNotPresent
          securityContext:
            runAsUser: 1000
            runAsGroup: 1000
          command: ["/bin/sh", "-c"]
          args:
            - |
              set -e
              echo "Starting keystore import process..."
              
              # Determine validator client type
              VC_TYPE="{{ .Values.validatorClient.type }}"
              echo "Validator client type: $VC_TYPE"
              
              # Check if keystores already exist in validator-data
              if [ "$VC_TYPE" = "lighthouse" ] && [ -f /validator-data/validators/validator_definitions.yml ]; then
                echo "Lighthouse validator_definitions.yml already exists, skipping import"
                exit 0
              elif [ "$VC_TYPE" = "lodestar" ] && [ -d /validator-data/keystores ] && [ -d /validator-data/secrets ] && [ "$(ls -A /validator-data/keystores 2>/dev/null)" ]; then
                echo "Lodestar keystores and secrets already exist, skipping import"
                exit 0
              elif [ "$VC_TYPE" = "teku" ] && [ -d /validator-data/keys ] && [ -d /validator-data/passwords ] && [ "$(ls -A /validator-data/keys 2>/dev/null)" ]; then
                echo "Teku keys and passwords already exist, skipping import"
                exit 0
              elif [ "$VC_TYPE" = "prysm" ] && [ -d /validator-data/wallet ] && [ -f /validator-data/wallet/direct/accounts/all-accounts.keystore.json ]; then
                echo "Wallet already exists, skipping import"
                exit 0
              elif [ "$VC_TYPE" = "nimbus" ] && [ -d /validator-data/validators ] && [ "$(ls -A /validator-data/validators 2>/dev/null)" ]; then
                echo "Keystores already exist in validator data, skipping import"
                exit 0
              fi
              
              # Determine source of keystores
              if [ -d /keystores ] && [ "$(ls -A /keystores 2>/dev/null)" ]; then
                echo "Found keystores in mounted secret"
                KEYSTORE_DIR="/keystores"
              elif [ -d /charon-data/validator_keys ] && [ "$(ls -A /charon-data/validator_keys 2>/dev/null)" ]; then
                echo "Found keystores from DKG output"
                KEYSTORE_DIR="/charon-data/validator_keys"
              else
                echo "ERROR: No keystores found to import"
                echo "Checked locations:"
                echo "  - /keystores (from secret): $(ls -la /keystores 2>&1 || echo 'Not mounted')"
                echo "  - /charon-data/validator_keys (from DKG): $(ls -la /charon-data/validator_keys 2>&1 || echo 'Not found')"
                exit 1
              fi
              
              echo "Importing keystores from: $KEYSTORE_DIR"
              
              # Import based on validator client type
              case "$VC_TYPE" in
                "lighthouse")
                  echo "Importing for Lighthouse..."
                  mkdir -p /validator-data/validators /validator-data/secrets
                  cp -v $KEYSTORE_DIR/keystore-*.json /validator-data/validators/ 2>/dev/null || echo "No keystore JSON files found"
                  cp -v $KEYSTORE_DIR/keystore-*.txt /validator-data/secrets/ 2>/dev/null || echo "No keystore password files found"
                  # Create validator_definitions.yml for Lighthouse
                  echo "---" > /validator-data/validators/validator_definitions.yml
                  for keystore in /validator-data/validators/keystore-*.json; do
                    if [ -f "$keystore" ]; then
                      keystore_name=$(basename "$keystore")
                      password_name=$(basename "$keystore" .json).txt
                      echo "- enabled: true" >> /validator-data/validators/validator_definitions.yml
                      echo "  voting_public_key: \"0x$(cat "$keystore" | grep -o '"pubkey": *"[^"]*"' | cut -d'"' -f4)\"" >> /validator-data/validators/validator_definitions.yml
                      echo "  type: local_keystore" >> /validator-data/validators/validator_definitions.yml
                      echo "  voting_keystore_path: /validator-data/validators/$keystore_name" >> /validator-data/validators/validator_definitions.yml
                      echo "  voting_keystore_password_path: /validator-data/secrets/$password_name" >> /validator-data/validators/validator_definitions.yml
                    fi
                  done
                  echo "Created validator_definitions.yml"
                  cat /validator-data/validators/validator_definitions.yml
                  chmod -R 400 /validator-data/validators/*.json 2>/dev/null || true
                  chmod -R 600 /validator-data/secrets/*.txt 2>/dev/null || true
                  chmod 600 /validator-data/validators/validator_definitions.yml
                  ;;
                  
                "lodestar")
                  echo "Importing for Lodestar (persisted keys backend)..."
                  mkdir -p /validator-data/keystores /validator-data/secrets
                  
                  # Create nested structure with per-keystore passwords
                  IMPORTED_COUNT=0
                  for f in $KEYSTORE_DIR/keystore-*.json; do
                    if [ ! -f "$f" ]; then
                      echo "No keystore files found"
                      continue
                    fi
                    
                    echo "Processing keystore: $f"
                    
                    # Extract pubkey from keystore JSON
                    PUBKEY="0x$(grep '"pubkey"' "$f" | awk -F'"' '{print $4}')"
                    
                    if [ -z "$PUBKEY" ] || [ "$PUBKEY" = "0x" ]; then
                      echo "ERROR: Could not extract pubkey from $f"
                      continue
                    fi
                    
                    # Create nested directory structure (one directory per validator pubkey)
                    PUBKEY_DIR="/validator-data/keystores/$PUBKEY"
                    mkdir -p "$PUBKEY_DIR"
                    
                    # Copy keystore with Lodestar's expected filename
                    cp "$f" "$PUBKEY_DIR/voting-keystore.json"
                    chmod 400 "$PUBKEY_DIR/voting-keystore.json"
                    
                    # Copy corresponding password file, named by pubkey
                    PASSWORD_FILE="${f%.json}.txt"
                    if [ -f "$PASSWORD_FILE" ]; then
                      cp "$PASSWORD_FILE" "/validator-data/secrets/$PUBKEY"
                      chmod 600 "/validator-data/secrets/$PUBKEY"
                      IMPORTED_COUNT=$((IMPORTED_COUNT + 1))
                      echo "Imported keystore for pubkey: $PUBKEY"
                    else
                      echo "ERROR: Password file not found: $PASSWORD_FILE"
                      exit 1
                    fi
                  done
                  
                  echo "Lodestar keystore import completed: imported=$IMPORTED_COUNT keystores"
                  ls -laR /validator-data/
                  ;;
                  
                "teku")
                  echo "Importing for Teku..."
                  mkdir -p /validator-data/keys /validator-data/passwords
                  cp -v $KEYSTORE_DIR/keystore-*.json /validator-data/keys/ 2>/dev/null || echo "No keystore JSON files found"
                  cp -v $KEYSTORE_DIR/keystore-*.txt /validator-data/passwords/ 2>/dev/null || echo "No keystore password files found"
                  chmod -R 400 /validator-data/keys/*.json 2>/dev/null || true
                  chmod -R 600 /validator-data/passwords/*.txt 2>/dev/null || true
                  ;;
                  
                "prysm")
                  echo "Importing for Prysm..."

                  # Just copy keystores to temporary location for now
                  # The actual import will be done in a separate Prysm init container
                  mkdir -p /validator-data/keystore-source
                  cp -v $KEYSTORE_DIR/keystore-*.json /validator-data/keystore-source/ 2>/dev/null || echo "No keystore JSON files found"
                  cp -v $KEYSTORE_DIR/keystore-*.txt /validator-data/keystore-source/ 2>/dev/null || echo "No keystore password files found"
                  chmod -R 400 /validator-data/keystore-source/*.json 2>/dev/null || true
                  chmod -R 600 /validator-data/keystore-source/*.txt 2>/dev/null || true

                  echo "Keystores copied. Prysm wallet creation and import will be done in next init container"
                  ;;
                  
                "nimbus")
                  echo "Importing for Nimbus..."

                  # Just copy keystores to temporary location for now
                  # The actual import will be done in a separate Nimbus init container
                  mkdir -p /validator-data/keystore-source
                  cp -v $KEYSTORE_DIR/keystore-*.json /validator-data/keystore-source/ 2>/dev/null || echo "No keystore JSON files found"
                  cp -v $KEYSTORE_DIR/keystore-*.txt /validator-data/keystore-source/ 2>/dev/null || echo "No keystore password files found"
                  chmod -R 400 /validator-data/keystore-source/*.json 2>/dev/null || true
                  chmod -R 600 /validator-data/keystore-source/*.txt 2>/dev/null || true

                  echo "Keystores copied. Nimbus import will be done in next init container"
                  ;;
                  
                *)
                  echo "ERROR: Unknown validator client type: $VC_TYPE"
                  exit 1
                  ;;
              esac
              
              echo "Keystore import completed successfully"
              echo "Imported keystores:"
              find /validator-data -name "*.json" -o -name "*.txt" | head -20
          volumeMounts:
            {{- if .Values.validatorClient.keystores.secretName }}
            - name: validator-keystores
              mountPath: /keystores
              readOnly: true
            {{- end }}
            - name: charon-data
              mountPath: /charon-data
              readOnly: true
            - name: validator-data
              mountPath: /validator-data

        # Prysm Wallet Creation and Keystore Import Init Container
        # Only runs when validator client type is Prysm
        # Follows Obol reference: https://github.com/ObolNetwork/charon-distributed-validator-node/blob/main/prysm/run.sh
        {{- if and .Values.validatorClient.enabled (eq .Values.validatorClient.type "prysm") }}
        - name: prysm-wallet-import
          image: {{ .Values.validatorClient.image.repository | default "gcr.io/prysmaticlabs/prysm/validator" }}:{{ .Values.validatorClient.image.tag | default "v7.1.0" }}
          command: ["/bin/sh", "-c"]
          args:
            - |
              set -e
              echo "=== Prysm Wallet Creation and Keystore Import ==="

              # Check if wallet already exists
              if [ -d "/validator-data/wallet" ] && [ -f "/validator-data/wallet/direct/accounts/all-accounts.keystore.json" ]; then
                echo "Wallet already exists, skipping import"
                exit 0
              fi

              # Check if keystores are available
              if [ ! -d "/validator-data/keystore-source" ] || [ ! "$(ls -A /validator-data/keystore-source/keystore-*.json 2>/dev/null)" ]; then
                echo "No keystores found in /validator-data/keystore-source"
                echo "Skipping Prysm wallet creation"
                exit 0
              fi

              # Generate wallet password
              WALLET_PASSWORD="prysm-wallet-$(head /dev/urandom | tr -dc A-Za-z0-9 | head -c 32)"
              echo "$WALLET_PASSWORD" > /validator-data/wallet-password.txt
              chmod 600 /validator-data/wallet-password.txt

              echo "Creating Prysm wallet..."
              /app/cmd/validator/validator wallet create \
                --wallet-dir=/validator-data/wallet \
                --wallet-password-file=/validator-data/wallet-password.txt \
                --keymanager-kind=direct \
                {{- if .Values.validatorClient.config.prysm.acceptTermsOfUse }}
                --accept-terms-of-use \
                {{- end }}
                || { echo "ERROR: Wallet creation failed"; exit 1; }

              echo "Wallet created successfully"

              # Create temporary directory for per-keystore import
              mkdir -p /tmp/tmpkeys

              # Import keystores one at a time with per-keystore passwords
              IMPORTED_COUNT=0
              for keystore_file in /validator-data/keystore-source/keystore-*.json; do
                if [ ! -f "$keystore_file" ]; then
                  continue
                fi

                echo "Importing keystore: $(basename $keystore_file)"

                # Copy keystore to temporary directory
                cp "$keystore_file" /tmp/tmpkeys/

                # Get corresponding password file
                password_file="${keystore_file%.json}.txt"
                if [ ! -f "$password_file" ]; then
                  echo "ERROR: Password file not found: $password_file"
                  exit 1
                fi

                # Import this specific keystore with its password
                /app/cmd/validator/validator accounts import \
                  --wallet-dir=/validator-data/wallet \
                  --keys-dir=/tmp/tmpkeys \
                  --account-password-file="$password_file" \
                  --wallet-password-file=/validator-data/wallet-password.txt \
                  {{- if .Values.validatorClient.config.prysm.acceptTermsOfUse }}
                  --accept-terms-of-use \
                  {{- end }}
                  || { echo "ERROR: Failed to import $(basename $keystore_file)"; exit 1; }

                # Remove the keystore from tmpkeys for next iteration
                rm /tmp/tmpkeys/$(basename "$keystore_file")

                IMPORTED_COUNT=$((IMPORTED_COUNT + 1))
                echo "Successfully imported $(basename $keystore_file)"
              done

              # Cleanup
              rm -rf /tmp/tmpkeys
              rm -rf /validator-data/keystore-source

              echo "=== Prysm import completed: $IMPORTED_COUNT keystores imported ==="
              echo "Wallet structure:"
              ls -laR /validator-data/wallet/
          volumeMounts:
            - name: validator-data
              mountPath: /validator-data
        {{- end }}

        # Nimbus Keystore Import Init Container
        # Only runs when validator client type is Nimbus
        # Follows Obol reference: https://github.com/ObolNetwork/charon-distributed-validator-node/blob/main/nimbus/run.sh
        {{- if and .Values.validatorClient.enabled (eq .Values.validatorClient.type "nimbus") }}
        - name: nimbus-import
          image: {{ .Values.validatorClient.image.repository | default "statusim/nimbus-eth2" }}:{{ .Values.validatorClient.image.tag | default "multiarch-v25.9.2" }}
          command: ["/bin/sh", "-c"]
          args:
            - |
              set -e
              echo "=== Nimbus Keystore Import ==="

              # Check if keystores already imported
              if [ -d "/validator-data/validators" ] && [ "$(ls -A /validator-data/validators 2>/dev/null)" ]; then
                echo "Validators already exist in data directory, skipping import"
                exit 0
              fi

              # Check if keystores are available
              if [ ! -d "/validator-data/keystore-source" ] || [ ! "$(ls -A /validator-data/keystore-source/keystore-*.json 2>/dev/null)" ]; then
                echo "No keystores found in /validator-data/keystore-source"
                echo "Skipping Nimbus import"
                exit 0
              fi

              # Create temporary directory for import
              tmpkeys="/tmp/tmpkeys"
              mkdir -p "$tmpkeys"

              # Import keystores one at a time with per-keystore passwords
              IMPORTED_COUNT=0
              for keystore_file in /validator-data/keystore-source/keystore-*.json; do
                [ -f "$keystore_file" ] || continue

                filename=$(basename "$keystore_file")
                echo "Importing keystore: $filename"

                # Get corresponding password file
                password_file="${keystore_file%.json}.txt"
                if [ ! -f "$password_file" ]; then
                  echo "ERROR: Password file not found: $password_file"
                  exit 1
                fi

                # Read password
                password=$(cat "$password_file")

                # Copy keystore to temporary directory
                cp "$keystore_file" "$tmpkeys/"

                # Import with password piped to stdin
                echo "$password" | /home/user/nimbus-eth2/build/nimbus_beacon_node deposits import \
                  --data-dir=/validator-data \
                  "$tmpkeys"

                # Remove the keystore from tmpkeys for next iteration
                rm "$tmpkeys/$filename"

                IMPORTED_COUNT=$((IMPORTED_COUNT + 1))
                echo "Successfully imported $filename"
              done

              # Cleanup
              rm -rf "$tmpkeys"
              rm -rf /validator-data/keystore-source

              echo "=== Nimbus import completed: $IMPORTED_COUNT keystores imported ==="
              echo "Validator directory contents:"
              ls -laR /validator-data/validators/ || echo "No validators directory yet"
          volumeMounts:
            - name: validator-data
              mountPath: /validator-data
        {{- end }}
        {{- end }}
      containers:
        - command:
            - sh
            - -ac
            - >
              exec /usr/local/bin/charon run
              --beacon-node-endpoints={{ include "dv-pod.beaconNodeEndpoints" . }}
              {{- if and .Values.charon.beaconNodeHeaders (not .Values.charon.beaconNodeHeadersSecretName) }}
              --beacon-node-headers={{ .Values.charon.beaconNodeHeaders | quote }}
              {{- end }}
              {{- if .Values.charon.fallbackBeaconNodeEndpoints }}
              --fallback-beacon-node-endpoints={{ include "dv-pod.fallbackBeaconNodeEndpoints" . }}
              {{- end }}
              {{- if .Values.charon.lockFile }}
              --lock-file={{ .Values.charon.lockFile }}
              {{- end }}
              {{- if .Values.charon.nickname }}
              --nickname={{ .Values.charon.nickname }}
              {{- end }}
              {{- if .Values.charon.builderApi }}
              --builder-api={{ .Values.charon.builderApi }}
              {{- end }}
              {{- if .Values.charon.featureSet }}
              --feature-set={{ .Values.charon.featureSet }}
              {{- end }}
              {{- if .Values.charon.featureSetDisable }}
              --feature-set-disable={{ .Values.charon.featureSetDisable }}
              {{- end }}
              {{- if .Values.charon.featureSetEnable }}
              --feature-set-enable={{ .Values.charon.featureSetEnable }}
              {{- end }}
              {{- if .Values.charon.logFormat }}
              --log-format={{ .Values.charon.logFormat }}
              {{- end }}
              {{- if .Values.charon.logLevel }}
              --log-level={{ .Values.charon.logLevel }}
              {{- end }}
              {{- if .Values.charon.lokiAddresses }}
              --loki-addresses={{ .Values.charon.lokiAddresses }}
              {{- end }}
              {{- if .Values.charon.lokiService }}
              --loki-service={{ .Values.charon.lokiService }}
              {{- else }}
              --loki-service="${NODE_NAME}"
              {{- end }}
              {{- if .Values.charon.monitoringAddress }}
              --monitoring-address=0.0.0.0:3620
              {{- end }}
              {{- if .Values.charon.noVerify }}
              --no-verify={{ .Values.charon.noVerify }}
              {{- end }}
              {{- if .Values.charon.p2pAllowlist }}
              --p2p-allowlist={{ .Values.charon.p2pAllowlist }}
              {{- end }}
              {{- if .Values.charon.p2pDenylist }}
              --p2p-denylist={{ .Values.charon.p2pDenylist }}
              {{- end }}
              {{- if .Values.charon.p2pDisableReuseport }}
              --p2p-disable-reuseport={{ .Values.charon.p2pDisableReuseport }}
              {{- end }}
              {{- if .Values.charon.directConnectionEnabled }}
              --p2p-external-hostname="${NODE_NAME}"
              {{- else }}
              --p2p-external-hostname={{ .Values.charon.p2pExternalHostname }}
              {{- end }}
              {{- if .Values.charon.p2pExternalIp }}
              --p2p-external-ip={{ .Values.charon.p2pExternalIp }}
              {{- end }}
              {{- if .Values.charon.p2pRelays }}
              --p2p-relays={{ .Values.charon.p2pRelays }}
              {{- end }}
              {{- if .Values.charon.p2pTcpAddress }}
              --p2p-tcp-address={{ .Values.charon.p2pTcpAddress }}
              {{- end }}
              --private-key-file="{{ .Values.charon.privateKeyFile }}"
              {{- if .Values.charon.validatorApiAddress }}
              --validator-api-address={{ .Values.charon.validatorApiAddress }}
              {{- end }}
          env:
          - name: NODE_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          {{- if .Values.charon.beaconNodeHeadersSecretName }}
          - name: CHARON_BEACON_NODE_HEADERS
            valueFrom:
              secretKeyRef:
                name: {{ .Values.charon.beaconNodeHeadersSecretName }}
                key: {{ .Values.charon.beaconNodeHeadersSecretKey }}
          {{- end }}
          image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
          name: {{ .Chart.Name }}
          {{- with .Values.containerSecurityContext }}
          securityContext:
            {{- toYaml . | nindent 12 }}
          {{- end }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: {{ .Values.service.ports.p2pTcp.targetPort }}
              name: p2p-tcp
              protocol: TCP
            - containerPort: 3620
              name: monitoring
              protocol: TCP
        {{- if .Values.livenessProbe.enabled }}
          livenessProbe:
            initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.livenessProbe.periodSeconds }}
            httpGet:
              path: {{ .Values.livenessProbe.httpGet.path }}
              port: {{ .Values.livenessProbe.httpGet.port }}
        {{- end }}
        {{- if .Values.readinessProbe.enabled }}
          readinessProbe:
            initialDelaySeconds: {{ .Values.readinessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.readinessProbe.periodSeconds }}
            httpGet:
              path: {{ .Values.readinessProbe.httpGet.path }}
              port: {{ .Values.readinessProbe.httpGet.port }}
        {{- end }}  
          volumeMounts:
            {{- if .Values.configMaps.clusterLock }}
            # Mount cluster-lock configMap if provided
            - name: cluster-lock
              mountPath: /charon-data/cluster-lock.json
              subPath: cluster-lock.json
              readOnly: true
            {{- end }}
            # Mount for the shared ENR private key
            - name: charon-enr-private-key
              mountPath: "{{ .Values.charon.privateKeyFile }}"
              subPath: "{{ include "dv-pod.enrSecretDataKey" . }}" # Mount the specific key file, not the whole secret dir
            - name: charon-data
              mountPath: /charon-data
              readOnly: true
          {{- with .Values.resources }}
          resources:
            {{- toYaml . | nindent 12 }}
          {{- end }}
        {{- if .Values.validatorClient.enabled }}
        # Validator Client Container
        - name: validator-client
          {{- if eq .Values.validatorClient.type "lighthouse" }}
          image: {{ .Values.validatorClient.image.repository | default "sigp/lighthouse" }}:{{ .Values.validatorClient.image.tag | default "v8.0.1" }}
          command:
            - lighthouse
            - validator_client
            - --beacon-nodes=http://localhost:3600
            - --suggested-fee-recipient=0x0000000000000000000000000000000000000000
            - --network={{ include "dv-pod.network" . }}
            - --graffiti={{ .Values.validatorClient.config.graffiti | quote }}
            - --datadir=/validator-data
            {{- if .Values.charon.builderApi }}
            - --builder-proposals
            {{- end }}
            - --init-slashing-protection
            - --metrics
            - --metrics-address=0.0.0.0
            - --metrics-port=5064
            - --distributed
            {{- range .Values.validatorClient.config.extraArgs }}
            - {{ . }}
            {{- end }}
          {{- else if eq .Values.validatorClient.type "teku" }}
          image: {{ .Values.validatorClient.image.repository | default "consensys/teku" }}:{{ .Values.validatorClient.image.tag | default "25.9.3" }}
          command:
            - /opt/teku/bin/teku
            - validator-client
            - --network={{ include "dv-pod.network" . }}
            - --beacon-node-api-endpoint=http://localhost:3600
            {{- if .Values.charon.builderApi }}
            - --validators-builder-registration-default-enabled=true
            {{- end }}
            - --validators-graffiti={{ .Values.validatorClient.config.graffiti | quote }}
            - --data-path=/validator-data
            - --validator-keys=/validator-data/keys:/validator-data/passwords
            - --validators-keystore-locking-enabled=false
            - --validators-external-signer-slashing-protection-enabled=true
            - --validators-proposer-default-fee-recipient=0x0000000000000000000000000000000000000000
            {{- if .Values.charon.executionClientRpcEndpoint }}
            - --ee-endpoint={{ .Values.charon.executionClientRpcEndpoint }}
            {{- end }}
            - --metrics-enabled=true
            - --metrics-interface=0.0.0.0
            - --metrics-port=5064
            - --Xobol-dvt-integration-enabled=true
            {{- range .Values.validatorClient.config.extraArgs }}
            - {{ . }}
            {{- end }}
          {{- else if eq .Values.validatorClient.type "prysm" }}
          image: {{ .Values.validatorClient.image.repository | default "gcr.io/prysmaticlabs/prysm/validator" }}:{{ .Values.validatorClient.image.tag | default "v7.1.0" }}
          command:
            - /app/cmd/validator/validator
            - --beacon-rest-api-provider=http://localhost:3600
            - --suggested-fee-recipient=0x0000000000000000000000000000000000000000
            # By using Prysm, you accept the Terms of Service: https://github.com/prysmaticlabs/prysm/blob/develop/TERMS_OF_SERVICE.md
            - --accept-terms-of-use
            {{- $network := include "dv-pod.network" . }}
            {{- if eq $network "hoodi" }}
            - --hoodi
            {{- else if eq $network "sepolia" }}
            - --sepolia
            {{- end }}
            - --graffiti={{ .Values.validatorClient.config.graffiti | quote }}
            {{- if .Values.charon.builderApi }}
            - --enable-builder
            {{- end }}
            - --datadir=/validator-data
            - --wallet-dir=/validator-data/wallet
            - --wallet-password-file=/validator-data/wallet-password.txt
            - --monitoring-host=0.0.0.0
            - --monitoring-port=5064
            - --distributed
            {{- range .Values.validatorClient.config.extraArgs }}
            - {{ . }}
            {{- end }}
            {{- range .Values.validatorClient.config.prysm.extraArgs }}
            - {{ . }}
            {{- end }}
          {{- else if eq .Values.validatorClient.type "nimbus" }}
          image: {{ .Values.validatorClient.image.repository | default "statusim/nimbus-validator-client" }}:{{ .Values.validatorClient.image.tag | default "multiarch-v25.9.2" }}
          command:
            - /home/user/nimbus_validator_client
            - --beacon-node=http://localhost:3600
            {{- if .Values.charon.builderApi }}
            - --payload-builder=true
            {{- end }}
            - --graffiti={{ .Values.validatorClient.config.graffiti | quote }}
            - --data-dir=/validator-data
            - --metrics
            - --metrics-address=0.0.0.0
            - --metrics-port=5064
            - --distributed
            {{- range .Values.validatorClient.config.extraArgs }}
            - {{ . }}
            {{- end }}
          {{- else if eq .Values.validatorClient.type "lodestar" }}
          image: {{ .Values.validatorClient.image.repository | default "chainsafe/lodestar" }}:{{ .Values.validatorClient.image.tag | default "v1.35.0" }}
          command:
            - node
            - /usr/app/packages/cli/bin/lodestar
            - validator
            - --beacon-nodes=http://localhost:3600
            - --network={{ include "dv-pod.network" . }}
            {{- if .Values.charon.builderApi }}
            - --builder="true" 
            - --builder.selection="builderonly"
            {{- end }}
            - --graffiti={{ .Values.validatorClient.config.graffiti | quote }}
            - --dataDir=/validator-data
            - --keystoresDir=/validator-data/keystores
            - --secretsDir=/validator-data/secrets
            - --metrics=true
            - --metrics.address=0.0.0.0
            - --metrics.port=5064
            - --distributed
            {{- range .Values.validatorClient.config.extraArgs }}
            - {{ . }}
            {{- end }}
          {{- end }}
          imagePullPolicy: {{ .Values.validatorClient.image.pullPolicy }}
          ports:
            - containerPort: 5064
              name: vc-metrics
              protocol: TCP
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
          volumeMounts:
            - name: charon-data
              mountPath: /charon-data
              readOnly: true
            - name: validator-data
              mountPath: /validator-data
          {{- with .Values.validatorClient.resources }}
          resources:
            {{- toYaml . | nindent 12 }}
          {{- end }}
        {{- end }}
      volumes:
        {{- if .Values.configMaps.clusterLock }}
        - name: cluster-lock
          projected:
            sources:
            - configMap:
                name: {{ .Values.configMaps.clusterLock }}
                optional: true
        {{- else }}
        - name: cluster-lock
          emptyDir: {}
        {{- end }}
        # Volume for the shared ENR private key and public ENR
        - name: charon-enr-private-key
          secret:
            secretName: {{ include "dv-pod.enrSecretName" . }}
            items:
              - key: {{ include "dv-pod.enrSecretDataKey" . }}
                path: {{ include "dv-pod.enrSecretDataKey" . }}
              - key: {{ include "dv-pod.enrSecretPublicDataKey" . }}
                path: {{ include "dv-pod.enrSecretPublicDataKey" . }}
        {{- if .Values.validatorClient.keystores.secretName }}
        # Volume for validator keystores secret
        - name: validator-keystores
          secret:
            secretName: {{ .Values.validatorClient.keystores.secretName }}
        {{- end }}
        {{- if not .Values.persistence.enabled }}
        # If persistence is disabled, use an emptyDir for charon-data.
        # Note: DKG artifacts will be lost if the pod restarts.
        - name: charon-data
          emptyDir: {}
        {{- end }}
        # Note: validator-data volume is always provided via PVC to prevent slashing
  volumeClaimTemplates:
    {{- if .Values.persistence.enabled }}
    - metadata:
        name: charon-data
        {{- with .Values.persistence.annotations }}
        annotations:
          {{- toYaml . | nindent 10 }}
        {{- end }}
      spec:
        accessModes: {{ .Values.persistence.accessModes | toYaml | nindent 10 }}
        resources:
          requests:
            storage: {{ .Values.persistence.size | quote }}
        {{- if .Values.persistence.storageClassName }}
        {{- if (eq "-" .Values.persistence.storageClassName) }}
        storageClassName: ""
        {{- else }}
        storageClassName: "{{ .Values.persistence.storageClassName }}"
        {{- end }}
        {{- end }}
    {{- end }}
    {{- if .Values.validatorClient.enabled }}
    # Validator data PVC is always created to prevent slashing
    - metadata:
        name: validator-data
        {{- with .Values.persistence.annotations }}
        annotations:
          {{- toYaml . | nindent 10 }}
        {{- end }}
      spec:
        accessModes: {{ .Values.persistence.accessModes | toYaml | nindent 10 }}
        resources:
          requests:
            storage: {{ .Values.persistence.validatorDataSize | default "500Mi" | quote }}
        {{- if .Values.persistence.storageClassName }}
        {{- if (eq "-" .Values.persistence.storageClassName) }}
        storageClassName: ""
        {{- else }}
        storageClassName: "{{ .Values.persistence.storageClassName }}"
        {{- end }}
        {{- end }}
    {{- end }}
