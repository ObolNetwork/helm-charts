
Charon Cluster
===========
{{ template "chart.deprecationWarning" . }}

{{ template "chart.versionBadge" . }}{{ template "chart.typeBadge" . }}{{ template "chart.appVersionBadge" . }}

{{ template "chart.description" . }}

{{ template "chart.homepageLine" . }}

{{ template "chart.sourcesSection" . }}

{{ template "chart.requirementsSection" . }}

# How to use this chart

A distributed validator cluster is composed of the following containers:

- Single execution layer client
- Single consensus layer client
- Number of Distributed Validator clients [This chart]
- Number of Validator clients
- Prometheus, Grafana and Jaeger clients for monitoring this cluster.

![Distributed Validator Cluster](https://github.com/ObolNetwork/charon-distributed-validator-cluster/blob/main/DVCluster.png?raw=true)

## Prerequisites

This chart deploys a single distributed validator pod. You have two options for providing the required artifacts:

### Option 1: Pre-created keys and artifacts (skips DKG process)

If you already have private keys and charon artifacts from a central key creation or a DKG ceremony, you can provide them to skip the automated DKG process entirely.

#### Example 1a: From a `charon create cluster` command output (a `./cluster/nodeN` folder structure)

When you've created keystores for a full cluster locally and have a `./cluster/` directory:

```
./cluster/
├── node0/
│   ├── charon-enr-private-key
│   ├── cluster-lock.json
│   └── validator_keys/
│       ├── keystore-0.json
│       └── keystore-0.txt
├── node1/
│   ├── charon-enr-private-key
│   ├── cluster-lock.json
│   └── validator_keys/
│       ├── keystore-0.json
│       └── keystore-0.txt
└── ...
```

To deploy node0 using this chart:
```sh
# Put the ENR private key in a secret
kubectl create secret generic charon-enr-private-key --from-file=cluster/node0/charon-enr-private-key

# Put the private keys in a kubernetes secret
kubectl create secret generic validator-keys \
  --from-file=cluster/node0/validator_keys/keystore-0.json --from-file=cluster/node0/validator_keys/keystore-0.txt

# Create the cluster lock ConfigMap (same for all nodes)
kubectl create configmap cluster-lock --from-file=cluster/node0/cluster-lock.json

# Install the chart, referencing your ConfigMap and Secrets
helm install my-dv-pod obol/dv-pod \
  --set configMaps.clusterLock=cluster-lock \
  --set validatorClient.keystores.secretName=validator-keys
```

That should be enough to get your DV-Pod running, and loaded with the required artifacts, assuming the default beacon node endpoint of the Obol Stack works. 

#### Example 1b: From a Distributed Key Generation ceremony output (a `.charon/` folder structure)

When you have DKG artifacts in a `.charon/` directory:

```
.charon/
├── charon-enr-private-key
├── cluster-lock.json
└── validator_keys/
    ├── keystore-0.json
    └── keystore-0.txt
```

Create the required resources:

```sh
# Put the charon ENR private key in a kubernetes secret
kubectl create secret generic charon-enr-private-key \
  --from-file=.charon/charon-enr-private-key

# Put the private keys in a kubernetes secret 
kubectl create secret generic validator-keys \
  --from-file=.charon/validator_keys/keystore-0.json --from-file=.charon/validator_keys/keystore-0.txt

# Put the cluster lock file in a kubernetes ConfigMap
kubectl create configmap cluster-lock \
  --from-file=.charon/cluster-lock.json

# Install the chart
helm install my-dv-pod obol/dv-pod \
  --set configMaps.clusterLock=cluster-lock \
  --set validatorClient.keystores.secretName=validator-keys
```

> [!IMPORTANT]
> The ENR secret MUST be created in the same namespace where you plan to install the Helm chart. The ENR job can only look for secrets in its own namespace.


### Option 2: Run DKG through the chart (automatic)

If you don't have pre-existing artifacts, the chart can automatically:
1. Generate an ENR private key (or use one you provide)
2. Run the DKG ceremony via the Obol API
3. Store the resulting artifacts in a persistent volume

Simply deploy the chart with your operator address:
```sh
helm install my-dv-pod obol/dv-pod \
  --set charon.operatorAddress=0xYOUR_OPERATOR_ADDRESS
```

The DKG sidecar will poll the Obol API for cluster invites and automatically run the DKG ceremony when ready.

## Add Obol's Helm Charts Repo

```sh
helm repo add obol https://obolnetwork.github.io/helm-charts
helm repo update
```
_See [helm repo](https://helm.sh/docs/helm/helm_repo/) for command documentation._

## Install the chart
Install a distributed validator pod:
```sh
helm upgrade --install my-dv-pod obol/dv-pod \
  --set='charon.beaconNodeEndpoints[0]=<BEACON_NODE_ENDPOINT>' \
  --set='charon.operatorAddress=<YOUR_OPERATOR_ADDRESS>'
```

## Validator Client

The dv-pod chart includes an integrated validator client that runs alongside Charon in the same pod. The validator client is automatically configured to connect to Charon's validator API.

### Supported Validator Clients

You can choose from the following validator clients using the `validatorClient.type` parameter:
- `lighthouse` (default)
- `teku`
- `nimbus`
- `lodestar`
- `prysm`

### Configuration Example

```yaml
validatorClient:
  enabled: true
  type: lighthouse
  config:
    graffiti: "My DV Pod"
    extraArgs:
      - --suggested-fee-recipient=0xYOUR_FEE_RECIPIENT_ADDRESS
```

### Validator Keystores

The chart supports two methods for providing validator keystores:

#### Option 1: Pre-existing Keystores (Recommended for Production)

If you have existing keystores, create a Kubernetes secret and reference it:

```sh
# Create secret with your keystores
kubectl create secret generic validator-keys \
  --from-file=keystore-0.json \
  --from-file=keystore-0.txt \
  --from-file=keystore-1.json \
  --from-file=keystore-1.txt

# Deploy the chart with the keystore secret
helm install my-dv-pod obol/dv-pod \
  --set validatorClient.keystores.secretName=validator-keys \
  --set configMaps.clusterLock=my-cluster-lock
```

#### Option 2: DKG-Generated Keystores (Automatic)

When running DKG through the chart, keystores are automatically generated and imported to the validator client. The import process handles the specific directory structure required by each validator client:

- **Lighthouse**: Keystores in `/validator-data/validators/`, passwords in `/validator-data/secrets/`
- **Lodestar**: Restructured with pubkey directories under `/validator-data/keystores/`
- **Teku**: Keystores in `/validator-data/keys/`, passwords in `/validator-data/passwords/`
- **Prysm**: Keystores in `/validator-data/wallets/`
- **Nimbus**: Similar to Lighthouse structure

## Namespace Requirements

### ENR Secret Management

The ENR (Ethereum Node Record) secret **MUST** be created in the same namespace as the Helm release. The chart's ENR job will only check for existing secrets within its own namespace (`.Release.Namespace`).

**Common Pitfall**: Creating the ENR secret in a different namespace (e.g., `dv-pod`) and then installing the chart in another namespace (e.g., `default`) will result in the ENR job generating a new ENR, potentially overriding your intended configuration.

**Best Practice**: Always ensure your secrets and Helm release are in the same namespace:

```bash
# Wrong approach - secret and chart in different namespaces
kubectl create secret generic charon-enr-private-key -n dv-pod --from-literal=...
helm install my-dv-pod obol/dv-pod  # Installs in default namespace - ENR will be regenerated!

# Correct approach - both in same namespace
kubectl create secret generic charon-enr-private-key -n dv-pod --from-literal=...
helm install my-dv-pod obol/dv-pod -n dv-pod  # Both in dv-pod namespace
```

## Advanced Usage

### Use an External Validator Client

While the dv-pod chart includes an integrated validator client, you may want to use an external validator client instead. To do this:

1. Disable the integrated validator client:
```yaml
validatorClient:
  enabled: false
```

2. Configure your external validator client to connect to the Charon node's validator API endpoint:
```
--beacon-node-api-endpoint="http://<RELEASE_NAME>-dv-pod.<NAMESPACE>.svc.cluster.local:3600"
```

For example, if you installed the chart as `my-dv-pod` in namespace `dv-pod`:
```
--beacon-node-api-endpoint="http://my-dv-pod.dv-pod.svc.cluster.local:3600"
```

Note: The Charon validator API on port 3600 provides the same interface as a beacon node API, allowing standard validator clients to connect without modification.

### Handling Large Cluster-Lock Files (>1MB)

If your `cluster-lock.json` file is larger than 1MB, you may encounter errors when creating the ConfigMap:

```sh
error validating data: ValidationError(ConfigMap.data.cluster-lock.json): invalid type for io.k8s.api.core.v1.ConfigMap.data: got "array", expected "string"
```

In this case, you have two options:

**Option A: Direct lock hash (Recommended)**

1. Extract the lock_hash from your cluster-lock.json:
   ```sh
   LOCK_HASH=$(jq -r '.lock_hash' cluster-lock.json)
   echo $LOCK_HASH
   ```

2. Install the chart with the lockHash value:
   ```sh
   helm install my-dv-pod obol/dv-pod \
     --set charon.lockHash=$LOCK_HASH \
     --set charon.operatorAddress=<YOUR_OPERATOR_ADDRESS>
   ```

**Option B: ConfigMap approach**

1. Extract the lock_hash and create a ConfigMap:
   ```sh
   LOCK_HASH=$(jq -r '.lock_hash' cluster-lock.json)
   kubectl create configmap cluster-lock-hash \
     --from-literal=lock-hash=$LOCK_HASH
   ```

2. Install the chart referencing the ConfigMap:
   ```sh
   helm install my-dv-pod obol/dv-pod \
     --set configMaps.lockHash=cluster-lock-hash \
     --set charon.operatorAddress=<YOUR_OPERATOR_ADDRESS>
   ```

The DKG sidecar will use the lock hash to fetch the full cluster lock from the Obol API.

## Uninstall the Chart
To uninstall and delete the `dv-pod` release:
```sh
helm uninstall dv-pod -n dv-pod
```
The command removes all the Kubernetes components associated with the chart and deletes the release.


## Full Chart Values Reference

{{ template "chart.valuesSection" . }}